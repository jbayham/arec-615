---
title: "Dynamic Programming Part B"
---



# Functional Equations and Value Function Properties

Dynamic programming problems often lead to **functional equations**—equations in which the *unknown* is a function rather than a scalar. The **Bellman equation** is the most important example in economics. Instead of solving for a scalar like $x$, we are solving for an entire function $V(\cdot)$ that satisfies a relationship involving itself.

---

## What Is a Functional Equation?

- A **functional equation** relates the value of a function at one point to its value at another point.  

- In its simplest form, it can look like:
$$
V(s) = F(s, V(g(s)))
$$
- where the function $V$ appears on both sides.  
- We are not trying to find a single number, but rather a function $V(\cdot)$ that makes this equation true for every $s$.

- Functional equations appear whenever we describe **recursive behavior** — situations where "the value of something today depends on the value of something tomorrow." 


A functional equation specifies a relationship between a function and its transformed version.

In dynamic optimization:
$$
V(s) = \max_{x \in X(s)} \{ \pi(s,x) + \beta V(f(s,x)) \}
$$
where:

- $V(s)$ is the **value function**, giving the maximum attainable value from state $s$.
- $\pi(s,x)$ is the **current payoff** (or profit, utility).
- $f(s,x)$ gives the **next-period state**.
- $\beta \in (0,1)$ is the **discount factor**.

The equation says: *The value of being in state $s$ today equals the current payoff plus the discounted value of the next state, assuming optimal choice $x$.*

------

### Functional Equations in Economics

**Example: A Simple Savings Problem**

Suppose an agent chooses consumption $c_t$ and next-period assets $a_{t+1}$.

$$
\begin{aligned}
&\max_{\{c_t,a_{t+1}\}_{t=0}^\infty} \sum_{t=0}^\infty \beta^t u(c_t) \\
&\text{s.t. } c_t + a_{t+1} = (1+r)a_t + y_t, \quad a_t \ge 0
\end{aligned}
$$

where:

- $u(c)$ is the utility function (e.g., $u(c)=\log(c)$),
- $r$ is the interest rate,
- $y_t$ is income,
- $\beta \in (0,1)$ is the discount factor.


Then the Bellman equation is
$$
V(a_t) = \max_{a_{t+1}\ge0} \{ u((1+r)a_t + y - a_{t+1}) + \beta V(a_{t+1}) \}.
$$
FOC (interior) with respect to $a_{t+1}$

Let $c_t=(1+r)a_t+y-a_{t+1}$. Then
$$
\frac{\partial}{\partial a_{t+1}}\Big[u((1+r)a_t + y - a_{t+1})+\beta V(a_{t+1})\Big]
=-u'(c_t)+\beta V'(a_{t+1})=0,
$$

The agent trades off **current utility** $u(c)$ versus the **future value** $\beta V(a')$.

-----

**Firm Investment Problem**

A firm with capital stock $k_t$ chooses investment $i_t$:



$$
\max_{i_t} \sum_{t=0}^\infty \beta^t \pi(k_t, i_t)
\quad \text{s.t.} \quad
k_{t+1} = (1-\delta)k_t + i_t, \quad k_t \ge 0.
$$

The recursive form:
$$
V(k_t) = \max_{i_t\ge0} \{ \pi(k_t, i_t) + \beta V((1-\delta)k_t + i_t) \}.
$$

The function $V(k_t)$ gives the value of capital $k_t$ as current profit plus discounted future (optimal) value of next-period capital. Again, the value function appears on both sides.

FOC with respect to $i_t$:
$$
\frac{\partial}{\partial i_t}\Big[\pi(k_t, i_t) + \beta V((1-\delta)k_t + i_t)\Big]
= \pi_{i}(k_t, i_t) + \beta V'((1-\delta)k_t + i_t) = 0.
$$


--------


**Resource Extraction Problem**

A resource owner decides how much to extract $x_t$ from stock $s_t$:
$$
V(s_t) = \max_{0\le x_t\le s_t} \{ p x_t - c(x_t) + \beta V(s_t - x_t) \}.
$$

Here:

- The **state** is remaining stock $s_t$.
- The **control** is extraction $x_t$.
- The **transition** is $s_{t+1} = s_t - x_t$.

The functional equation states that the value of the resource today equals profit from extraction plus the discounted value of what remains for tomorrow.


------

## The Bellman Operator

The Bellman equation tells us that the value of being in a given state today, $V(s)$, equals the best possible current payoff plus the discounted value of what happens next.

We can think of this process as an **operator** — a kind of "machine" that takes a guess about the value function and produces a new, updated guess.


### How it works

Start with any function $V(s)$ that tells you what the value might be for each state.  
Then define a new function:
$$
(\mathcal{T}V)(s) = \max_{x \in X(s)} \big\{ \pi(s,x) + \beta V(f(s,x)) \big\}.
$$

What this says in words:

> “Given my current guess about how valuable future states are (that’s $V$), what would be the total value of making the best decision today?”

So $\mathcal{T}$ takes the *old* value function and gives you a *new* one that’s a little closer to the truth.  
It’s a way to **think one step ahead**.


### The Fixed Point Idea

The true value function, $V^*(s)$, is the one that **doesn’t change** when we apply this operation again:

$$
V^*(s) = (\mathcal{T}V^*)(s).
$$

In other words, if you already know the correct $V^*$, thinking one step ahead doesn’t change your beliefs because you are already correct about the future.

Why this matters

- The operator gives us a **recipe for computing** $V^*$: 
    - start with a guess and keep applying $\mathcal{T}$ repeatedly.
    - Each time, you’re improving your estimate of the value of being in each state.
- Economically, this process mirrors **learning or planning**:  
    - we evaluate today’s decisions using our expectations of tomorrow, adjust, and repeat until everything is internally consistent.

Visual Intuition

- If you plotted $V_0(s)$ (your first guess) and then $V_1(s) = \mathcal{T}V_0(s)$, the curves would move closer and closer together until they line up at $V^*(s)$.  

- That’s what it means for the Bellman equation to be a **fixed point** — a steady state in your expectations about value.

In the context of economics

- For a **consumer**, $\mathcal{T}$ means re-evaluating how much future consumption is worth.  
- For a **firm**, it means updating the expected profitability of holding or investing capital.  
- For a **resource owner**, it means revising how valuable it is to leave part of the stock for tomorrow.

In all cases, the Bellman operator captures the logic of **forward-looking behavior**:  
today’s value depends on how optimally we plan for tomorrow.

Each application of $\mathcal{T}$ corresponds to "thinking one step further ahead."

- Starting with any initial guess $V_0(s)$,
- Repeatedly applying $\mathcal{T}$, $V_{k+1} = \mathcal{T}V_k$,
- Converges to the true value function $V^*$ under mild conditions.

This is **Value Function Iteration (VFI)**.

-----

## Existence and Uniqueness: Why the Bellman Equation Has a Single Solution

Once we define the Bellman operator $\mathcal{T}$ - the rule that takes a guess about future value and updates it - we can ask two key questions:

1. Does this process always lead to a stable value function?
2. Will it always settle on the **same** function, no matter where we start?

The answer is yes — as long as future payoffs are **discounted** (so $\beta < 1$).  

The reason is the **contraction mapping property**.

A **contraction mapping** is a transformation that *pulls things closer together* every time you apply it.  

$$
|| \mathcal{T}V_1 - \mathcal{T}V_2 || \leq \gamma \, || V_1 - V_2 ||
$$

Imagine taking two different guesses about the value function, say $V_1(s)$ and $V_2(s)$. When we apply the Bellman operator to both, the resulting functions $\mathcal{T}V_1$ and $\mathcal{T}V_2$ are **closer to each other** than the originals.

Each round of updating reduces the distance between our guesses — eventually, all sequences converge to the same point, the **true value function** $V^*$.

Discounting is what makes this work. Because future rewards are multiplied by $\beta < 1$, any disagreement about the future is automatically **shrunk** when we think one step ahead.

Small differences in how we value the future can’t explode backward into large differences today — they fade over time.

This gives the Bellman operator its *gravitational pull* toward a single stable value function.

If applying $\mathcal{T}$ repeatedly always pulls guesses closer together, there must be one and only one function that can’t be improved upon —  that’s the **fixed point**:
$$
V^*(s) = \mathcal{T}V^*(s).
$$

Mathematically, this follows from the **Contraction Mapping Theorem**, but economically, you can think of it as saying:

There is one internally consistent way to value the future that agrees with itself when we plan forward.

### How this helps us in practice

Because $\mathcal{T}$ is a contraction, we can find $V^*$ by **value function iteration**:

1. Start with any initial guess $V_0(s)$ — even something crude.
2. Apply $\mathcal{T}$ to get an updated guess $V_1 = \mathcal{T}V_0$.
3. Keep repeating: $V_{n+1} = \mathcal{T}V_n$.

Each iteration gets us closer to the truth.  
No calculus tricks, no global search — just forward iteration guided by economic logic.

### The big takeaway

- Discounting and diminishing returns make the future “well-behaved.”  
- Together they guarantee that the Bellman equation has **one** solution, and that repeated forward-looking reasoning will find it.
- This is why we can compute dynamic equilibria with confidence: as long as the problem is discounted and well-behaved, there is a single, stable value function waiting to be found.

------

## Euler Equations and the Envelope Condition

Dynamic optimization gives two complementary characterizations of optimal behavior:

- **Euler equation (FOC in the control):** how the agent trades off current vs. future returns when choosing $x$.
- **Envelope condition (FOC “in the state”):** how the lifetime value $V$ changes with the state $s$.

The envelope condition is what lets us **eliminate messy derivatives of the policy function** and express the Euler equation in terms of primitives and $V'$ only.

We work with the same notation as above:
$$
V(s)=\max_{x\in X(s)}\{\pi(s,x)+\beta V(f(s,x))\},\quad 0<\beta<1,
$$
with optimal policy $x^*(s)$.

### From Bellman to First-Order Conditions

Assume an interior, differentiable solution for intuition (we add bounds/KKT below).

- **Stationarity (optimum in $x$):**
$$
\pi_x(s,x^*(s))+\beta\,V'(f(s,x^*(s)))\,f_x(s,x^*(s))=0.
$$

- **Envelope (optimum in $s$):**
$$
V'(s)=\pi_s(s,x^*(s))+\beta\,V'(f(s,x^*(s)))\,f_s(s,x^*(s)).
$$

**Why envelope works:** when differentiating the max wrt $s$, the chain-rule term involving $x_s^*(s)$ vanishes because the stationarity condition sets the derivative wrt $x$ to zero at the optimum. Intuitively, a marginal change in $s$ doesn’t induce a first-order change through the (already optimized) $x^*(s)$.

### The (One-Step-Ahead) Euler Equation

Write the time-$t$ Bellman equation at $(s_t,x_t)$ with $s_{t+1}=f(s_t,x_t)$:
$$
\pi_x(s_t,x_t)+\beta\,V'(s_{t+1})\,f_x(s_t,x_t)=0.
$$

This is the **Euler equation**: the current marginal payoff from $x_t$ equals the discounted marginal value of how $x_t$ moves the state into the future.

We can **eliminate $V'(s_{t+1})$** using the envelope condition at $t+1$:
$$
V'(s_{t+1})=\pi_s(s_{t+1},x_{t+1})+\beta\,V'(s_{t+2})\,f_s(s_{t+1},x_{t+1}),
$$
which yields a purely **primitive** intertemporal tradeoff once substituted back. In many applications $\pi$ does not depend directly on $s$ (only through feasibility), making this especially clean.



------

#### Consumption–Savings (simple, separable)

Let’s illustrate with the **savings–consumption problem**.

The household chooses next period’s assets $a_{t+1}$ each period:
$$
V(a_t) = \max_{a_{t+1} \ge 0} \Big\{ u(c_t) + \beta V(a_{t+1}) \Big\},
$$
subject to the budget constraint
$$
c_t + a_{t+1} = (1+r)a_t + y.
$$

Substitute $c_t = (1+r)a_t + y - a_{t+1}$, so
$$
V(a_t) = \max_{a_{t+1} \ge 0} \big\{ u((1+r)a_t + y - a_{t+1}) + \beta V(a_{t+1}) \big\}.
$$

**The First-Order Condition (Euler Equation)**

Take the derivative of the Bellman RHS with respect to $a_{t+1}$:
$$
- u'(c_t) + \beta V'(a_{t+1}) = 0,
$$
which gives
$$
u'(c_t) = \beta V'(a_{t+1}).
$$
This is the **Euler condition** in implicit form: the marginal utility today equals the discounted shadow value of next period’s assets.

**The Envelope Condition**

Differentiate the Bellman equation with respect to $a_t$, treating $a_{t+1}$ as constant (by the envelope theorem):
$$
V'(a_t) = u'(c_t)(1+r). 
$$

Intuitively: a marginal increase in assets $a_t$ raises consumption by $(1+r)$, increasing utility by $u'(c_t)(1+r)$.

::: {.callout-note}

# Why We Can “Treat $a_{t+1}$ as Constant”
$$
\frac{dV(a_t)}{da_t}
= u'(c_t)\big[(1+r) - \frac{da_{t+1}^*(a_t)}{da_t}\big]
  + \beta V'(a_{t+1}) \frac{da_{t+1}^*(a_t)}{da_t}.
$$

Now, note that the **first-order condition** for optimal $a_{t+1}$ is:
$$
-\,u'(c_t) + \beta V'(a_{t+1}) = 0.
$$

Rearrange and substitute this into the derivative:
$$
\frac{dV(a_t)}{da_t}
= u'(c_t)(1+r)
- \big[u'(c_t) - \beta V'(a_{t+1})\big]\frac{da_{t+1}^*(a_t)}{da_t}.
$$

The term in brackets equals zero by the FOC, so the entire last term drops out:
$$
V'(a_t) = u'(c_t)(1+r).
$$
:::

We can use the envelope condition one period ahead
$$
V'(a_{t+1}) = u'(c_{t+1})(1+r).
$$

Then substitute it into the original FOC wrt $a_{t+1}$

$$
u'(c_t) = \beta (1+r) u'(c_{t+1}).
$$

This is the **standard consumption Euler equation**, expressing intertemporal optimality purely in terms of marginal utilities and the return $1+r$.

**Economic interpretation**

The Euler equation equates the *marginal benefit* and *marginal cost* of saving.

- The left-hand side, $u'(c_t)$, is the **marginal utility cost** of giving up one unit of consumption today.
- The right-hand side, $\beta(1+r)u'(c_{t+1})$, is the **discounted marginal utility benefit** of the extra consumption made possible tomorrow by saving that unit and earning the return $(1+r)$.

In equilibrium, these two must be equal — meaning that the household is indifferent (at the margin) between consuming one more unit today or saving it to consume tomorrow.

**Interpreting the ratio of marginal utilities**

Rearranging gives
$$
\frac{u'(c_{t+1})}{u'(c_t)} = \frac{1}{\beta(1+r)}.
$$

- The **left-hand side** is the *intertemporal marginal rate of substitution* (IMRS) — the rate at which the household is willing to trade future consumption for current consumption.
- The **right-hand side** is the *intertemporal price ratio* implied by markets — the relative price of tomorrow’s consumption in terms of today’s, discounted by $\beta$ and adjusted for the gross interest rate $(1+r)$.

In equilibrium, these two ratios are equal: the household’s willingness to substitute consumption across periods equals the market’s rate of return on savings.

**Intuition**

- If the interest rate $r$ rises, the right-hand side decreases.  
  To restore equality, the ratio $u'(c_{t+1})/u'(c_t)$ must fall — meaning $c_{t+1}/c_t$ rises.  
  → **Higher interest rates encourage saving and future consumption.**

- If $\beta$ falls (the agent becomes more impatient), the right-hand side decreases.  
  → **The household consumes more today and less tomorrow.**

This condition captures the **core intertemporal tradeoff** at the heart of dynamic economic behavior:  
balancing impatience, returns, and the curvature of utility (risk aversion or diminishing marginal utility).

Let's assume constant relative risk aversion
$$
u(c) = \frac{c^{1-\sigma}}{1-\sigma}, \quad \sigma > 0,
$$
where $\sigma$ is the **coefficient of relative risk aversion** (and $1/\sigma$ is the **intertemporal elasticity of substitution**).

Then
$$
u'(c) = c^{-\sigma}.
$$

Plug this into the Euler equation:
$$
u'(c_t) = \beta (1+r) u'(c_{t+1})
\quad \Rightarrow \quad
c_t^{-\sigma} = \beta (1+r) c_{t+1}^{-\sigma}.
$$

Rearrange for the **growth rate of consumption**:
$$
\frac{c_{t+1}}{c_t}
= \big[\,\beta(1+r)\,\big]^{1/\sigma}.
$$



**Interpretation**

- If $\beta(1+r)=1$, then $c_{t+1}=c_t$: consumption is *constant* over time.
- If $\beta(1+r)>1$, the household values future consumption relatively more → **consumption grows** over time.
- If $\beta(1+r)<1$, the household is relatively impatient → **consumption declines** over time.

**Economic parameters:**
- $\beta$ captures *patience*: higher $\beta$ → slower consumption decline (more saving).
- $\sigma$ governs *willingness to smooth consumption*: higher $\sigma$ (more curvature) → less sensitivity of growth to interest rates.

This simple form makes the Euler condition directly testable and provides a foundation for empirical work in both **macroeconomics** and **household finance**.

#### Renewable Resource (fishery; matches your notes)

Bellman: $V(s)=\max_{0\le h\le s}\{p\,h-c\,h+\beta V(G(s-h))\}$ with $s'=G(s-h)$.

- **Euler (interior):**
$$
(p-c)-\beta\,V'(s')\,G'(s-h)=0.
$$

- **Envelope:**
$$
V'(s)=\beta\,V'(s')\,G'(s-h).
$$

Combine them to eliminate $V'(s')$:
$$
p-c=V'(s).
$$

**Interpretation:** harvest until marginal net revenue equals the **shadow value** of the stock (the user cost). With bounds, you get the usual “escapement” logic: at low $s$, $h^*=0$ (rebuilding); at high $s$, $h^*$ hits the upper bound $h=s$ (if profitable).


------

## Analytical Example: Hotelling Resource Extraction

We now work through a fully analytical example that ties together the **Euler equation** and the **Envelope (shadow value)** condition.

### Setup

A social planner (or resource owner) chooses extraction $\{q_t\}$ to maximize the discounted value of profits from a nonrenewable resource.

$$
\max_{\{q_t\}_{t=0}^\infty} \sum_{t=0}^\infty \beta^t \big[p\,q_t - c(q_t)\big]
$$

subject to the **resource stock constraint**
$$
S_{t+1} = S_t - q_t, \qquad S_t \ge 0,
$$
with given $S_0$ and discount factor $0<\beta<1$.



### Bellman Equation

Let $V(S)$ be the value of having $S$ units of the resource left:

$$
V(S) = \max_{0\le q\le S} \left\{ p\,q - c(q) + \beta\,V(S-q) \right\}.
$$



### First-Order and Envelope Conditions

#### (a) Euler (stationarity in $q$)

$$
p - c'(q_t) = \beta\,V'(S_{t+1}).
$$

The planner extracts up to the point where the **current net price** equals the **discounted shadow value of remaining stock**.

#### (b) Envelope (marginal value of the stock)

Differentiating the Bellman equation with respect to $S$ and using the envelope theorem:

$$
V'(S_t) = \beta\,V'(S_{t+1}).
$$



### Combine Euler and Envelope

Shift the envelope one period forward:
$$
V'(S_{t+1}) = \beta\,V'(S_{t+2}).
$$

Substitute into the Euler:
$$
p - c'(q_t) = \beta\,V'(S_{t+1}) = \beta^2 V'(S_{t+2}) = \cdots
$$
so $V'(S_t)$ grows at the rate $1/\beta$.

Define $\lambda_t \equiv V'(S_t)$ as the **shadow price** or **resource rent**. Then:
$$
\lambda_{t+1} = \frac{\lambda_t}{\beta}.
$$

If $\beta = (1+r)^{-1}$, this gives the classic **Hotelling rule**:
$$
\lambda_{t+1} = (1+r)\lambda_t.
$$

That is, the *resource rent* (or in equilibrium, the **net price**) must grow at the rate of interest.

---

### Linear-Cost Example

Let $c(q)=\tfrac{1}{2}\gamma q^2$ with $\gamma>0$.  
Then the FOC is
$$
p - \gamma q_t = \beta V'(S_{t+1}).
$$

Using the envelope $V'(S_t)=\beta V'(S_{t+1})$, we can eliminate $V'$:

$$
p - \gamma q_t = V'(S_t).
$$

Differentiate over time to get the *Euler equation in observable variables*:
$$
(p - \gamma q_{t+1}) = \frac{1}{\beta}(p - \gamma q_t).
$$

If $\beta=(1+r)^{-1}$,
$$
p - \gamma q_{t+1} = (1+r)(p - \gamma q_t),
$$
so the **net price (marginal rent)** rises at the interest rate.

---

### 6. Economic Interpretation

| Condition | Meaning | Intuition |
|------------|----------|-----------|
| $p - c'(q_t) = \beta V'(S_{t+1})$ | *Euler:* marginal profit = discounted shadow value | Extract until current profit = opportunity cost of future scarcity |
| $V'(S_t)=\beta V'(S_{t+1})$ | *Envelope:* shadow price grows at $1/\beta$ | Keeping one more unit today increases lifetime value by the discounted rent |
| $\lambda_{t+1}=(1+r)\lambda_t$ | *Hotelling rule* | In equilibrium, the resource rent (or net price) grows at the interest rate |

---

### Policy Intuition

- When the interest rate $r$ is **high**, future rents are heavily discounted → faster extraction.  
- When $r$ is **low** or $\beta$ high, future rents matter more → slower extraction.  
- The rule does *not* depend on the level of stock $S_t$; it is purely an **arbitrage condition** between leaving the resource in the ground vs. extracting and investing the proceeds.


:::{.callout-note}
Why Must Rents Grow at the Interest Rate?

The Hotelling rule says that in equilibrium,
$$
\lambda_{t+1} = (1+r)\lambda_t,
$$
or equivalently that the **net price (resource rent)** grows at the rate of interest.

Let’s unpack *why* this must be true.


**The Resource as an Asset**

Think of the remaining stock $S_t$ as an **asset**: each unit in the ground is a claim on a future profit when extracted.

- If you **extract one more unit now**, you earn today’s net revenue  
  $\text{current rent} = p - c'(q_t) = \lambda_t$.

- If you **leave it in the ground**, you keep the option to sell it in the future, when scarcity will be higher and rent will be larger.

Thus, each unit of resource is an asset that yields **capital gains** (through rising rent) but **no dividends** (unless extracted).


**No-Arbitrage Condition**

An owner can hold wealth in two forms:

1. **Financial asset** yielding interest rate $r$.  
2. **Resource asset** whose value (the rent $\lambda_t$) appreciates over time.

If both are riskless, **arbitrage** implies they must yield the same return.

Otherwise:

- If $\lambda_{t+1}/\lambda_t > 1+r$:  
  holding the resource yields higher return → everyone hoards → no extraction.
- If $\lambda_{t+1}/\lambda_t < 1+r$:  
  financial assets dominate → everyone extracts and invests proceeds.

Only when $\lambda_{t+1}/\lambda_t = 1+r$ can both be held in equilibrium.

That is exactly the **Hotelling condition**:
$$
\frac{\lambda_{t+1}-\lambda_t}{\lambda_t} = r.
$$

:::

\newpage

# Continuous Time and State

Like in many other contexts, continuous processes can simplify or allow the use of other math. By moving to continuous time, we can use calculus.

Instead of difference equations, we use differential equations to describe evolution of the state variable.

$$
V(s)=\max_x  \int_0^\infty e^{−\rho t}π(s(t),x(t)) dt
$$
$$
\text{s.t.} ~ \dot s(t)=f(s(t),x(t))
$$

## Current Value Hamiltonian

The current value hamiltonian is the dynamic analog to the Lagrangian in static optimization. It combines current payoffs and the value of changing the state.

$$ 
H(s,x,\lambda) = \pi (s,x) + \lambda f(s,x)
$$

where $\lambda(t)$ is the co-state variable or shadow price associated with the state $s(t)$ in current value terms.

### FOC (Pontryagin maximum principle)

The necessary first order conditions are

- Control equation: $\frac{\partial H}{\partial x} = 0$
- Co-state equation: $\dot \lambda = \rho \lambda - \frac{\partial H}{\partial s}$
- State equation: $\dot s = f(s,x)$
- Transversality condition: $\lim_{t\rightarrow \infty} e^{-\rho t} \lambda(t) s(t) = 0$

**Control Condition**

$$
\frac{\partial H}{\partial x} = \frac{\partial \pi(s,x)}{\partial x} + \lambda \frac{\partial f(s,x)}{\partial x} = 0
$$
This condition equates the marginal benefit and marginal cost of adjusting the control variable $x$.

- $\frac{\partial \pi}{\partial x}$ is the direct marginal profit from increasing $x$.

- $\lambda \frac{\partial f}{\partial x}$ is the indirect effect through how $x$ changes the state variable — multiplied by the shadow value of the state.

So at the optimum the immediate gain from changing $x$ must equal the value of the induced change in the future state.

**Co-state equation**

The condition $\dot\lambda = \rho\lambda - H_s$ says that shadow values evolve like an asset price — their rate of appreciation equals the interest rate minus the marginal productivity of the state.

$$
\dot \lambda = \rho \lambda - \frac{\partial H}{\partial s} = \rho \lambda - \left( \frac{\partial \pi}{\partial s} + \lambda \frac{\partial f}{\partial s} \right)
$$

This is a law of motion for the shadow price — it tells us how the value of the state evolves over time.

- The term $\rho \lambda$ is the “required return” on the shadow asset (the discounting effect).

- $\frac{\partial \pi}{\partial s}$ is the direct marginal effect of more $s$ on current profit.

- $\lambda \frac{\partial f}{\partial s}$ is the indirect effect through how $s$ changes its own future growth.

**State equation**

This describes the evolution of the physical or economic state.
$$\dot{s} = f(s,x)$$
The chosen control policy $x(t)$ determines how the system transitions through states — e.g. how capital accumulates, resources deplete, or pollution stock evolves. This is the link between short-run choices and long-run consequences.

The FOCs of the current-value Hamiltonian are the continuous-time counterpart of the Euler equation and envelope condition in discrete-time dynamic programming:

- The control equation parallels the Euler condition: it characterizes optimal intertemporal allocation.

- The co-state equation mirrors the envelope theorem: it tells how the value function changes with the state.

- Together, they describe how current actions balance immediate payoffs and discounted future consequences.

:::{.callout-note}
# Where does the Co-state equation come from

1. We introduce a Lagrange multiplier $\mu(t)$ for the constraint $\dot{s} - f(s,x)=0$.

The present-value Lagrangian is:
$$
\mathcal{L} = \int_0^\infty e^{-\rho t} \left[ \pi(s,x) + \mu(t) (\dot{s} - f(s,x)) \right] dt
$$
2. Integrate the term with $\dot{s}$ by parts:
To get the first-order conditions, we want all terms multiplied by $s$ (not $\dot{s}$).
$$
\int_0^\infty e^{-\rho t} -\mu(t) \dot s(t) dt = \left[ e^{-\rho t} \mu(t) s(t) \right]_0^\infty + 
\int_0^\infty e^{-\rho t} s(t) \left( \dot \mu(t) - \rho \mu(t) \right) dt
$$
Assuming the boundary term vanishes at infinity, we have:
$$\mathcal{L} = \int_0^\infty e^{-\rho t} \left[ \pi(s,x) - \mu(t) f(s,x) + s(t) \left( \dot{\mu}(t) - \rho \mu(t) \right) \right] dt$$

3. Define the current-value co-state variable:
$$\lambda(t) = \frac{\mu(t)}{e^{-\rho t}} = e^{\rho t} \mu(t)$$
This rescales the multiplier to account for discounting.

Differentiating $\lambda(t)$ and rearranging gives:
$$\dot \mu(t) = e^{-\rho t} ( \dot \lambda(t) - \rho \lambda(t))$$
When we plug this back into the FOCs, the necessary condition for $s(t)$ (the “state variable”) gives:
$$\dot \lambda(t) = \rho \lambda(t) - \left( \frac{\partial \pi}{\partial s} + \lambda(t) \frac{\partial f}{\partial s} \right)$$
This is the **co-state equation** we wanted to derive

:::

## Connecting the Hamiltonian to the HJB Equation

The Hamilton-Jacobi-Bellman (HJB) equation is the continuous-time analog of the Bellman equation in discrete time. It characterizes the value function $V(s)$ as the solution to a differential equation.

The HJB equation states that at each instant in time, the value of being in state $s$ equals the maximum of current payoffs plus the expected change in value from moving to a new state.

$$\rho V(s) = \max_x \left\{ \pi(s,x) + V'(s) f(s,x) \right\}$$

### Define the value function

$$
V(s) = \max_{x} \int_0^\infty e^{-\rho t} \pi(s(t), x(t)) dt
$$
At any time $t$, define the remaining value of the program:
$$V(s(t)) = \max_{x(\cdot)} \int_t^\infty e^{-\rho (u-t)} \pi(s(u), x(u)) du$$
Differentiating with respect to time $t$ gives:
$$\frac{dV(s(t))}{dt} = \rho V(s(t)) - \pi(s(t), x(t)) - V'(s(t)) \dot s(t)$$

:::{.callout-note}
# Leibniz Rule for Differentiation Under the Integral Sign

If you have an integral with variable limits:
$$
I(t) = \int_{a(t)}^{b(t)} g(u,t) du
$$
The derivative with respect to $t$ is:
$$\frac{dI(t)}{dt} = g(b(t),t)  b'(t) - g(a(t),t) \cdot a'(t) + \int_{a(t)}^{b(t)} \frac{\partial g(u,t)}{\partial t} du$$
In our case, the lower limit is $t$ and the upper limit is $\infty$. The derivative becomes:
$$\frac{d}{dt} \int_t^\infty e^{-\rho (u-t)} \pi(s(u), x(u)) du
= - e^{-\rho (t-t)} \pi(s(t), x(t)) + \int_t^\infty \frac{\partial}{\partial t} \left( e^{-\rho (u-t)} \pi(s(u), x(u)) \right) du$$

Note that $t-t=0$ so $e^{-\rho (t-t)}=1$ and

$$
\frac{\partial}{\partial t} \left( e^{-\rho (u-t)} \pi(s(u), x(u)) \right)
= \rho e^{-\rho (u-t)} \pi(s(u), x(u)) 
= \rho V(s(t))
$$
:::

Note that $V(s(t))$ is the maximum value, so the derivative must be zero at the optimum:
$$0 = \rho V(s(t)) - \pi(s(t), x(t)) - V'(s(t)) \dot s(t)$$
Rearranging gives the HJB equation:
$$\rho V(s) = \pi(s,x) + V'(s) f(s,x)$$
Maximizing over $x$ gives the full HJB:
$$\rho V(s) = \max_x \left\{ \pi(s,x) + V'(s) f(s,x) \right\}$$

### Link to the Hamiltonian

The value function is linked to the co-state variable by:
$$\lambda(s) = V'(s)$$

## Hotelling Resource Extraction in Continuous Time

Consider a resource owner extracting a nonrenewable resource over time to maximize discounted profits:
$$
\max_{\{q(t)\}_{t=0}^\infty} \int_0^\infty e^{-\rho t} \left[ p q(t) - c(q(t)) \right] dt
$$

subject to the resource stock constraint:
$$
\dot S(t) = - q(t), \quad S(t) \ge 0, \quad S(0) = S_0
$$

### Current-Value Hamiltonian

The current-value Hamiltonian is:
$$H(S,q,\lambda) = p q - c(q) + \lambda (-q) = (p - \lambda) q - c(q)$$

### First-Order Conditions  

- **Control (optimal extraction):**
$$\frac{\partial H}{\partial q} = p - c'(q) - \lambda = 0 \quad \Rightarrow \quad p - c'(q) = \lambda$$
This condition states that the marginal profit from extraction equals the shadow price of the resource.
- **Co-state (shadow price evolution):**
$$\dot \lambda = \rho \lambda - \frac{\partial H}{\partial S} = \rho \lambda - 0 = \rho \lambda$$
Since $H$ does not depend on $S$, the shadow price grows at the rate $\rho$.    
$\lambda$ is the shadow value of the resource in situ, i.e. the resource rent.    
It must grow at the rate of interest $\rho$.    
Thus, resource rents rise at the interest rate — the Hotelling rule.    
If $\dot{\lambda} = \rho \lambda$, rents increase exponentially — extraction slows as the resource becomes scarcer.


## Numeric Example


### Hotelling extraction with quadratic costs (continuous time)

We consider the nonrenewable resource problem in continuous time with constant price and quadratic extraction costs:

- Objective: maximize discounted profits
$$
\max_{\{x(t)\}_{t\ge0}} \int_0^\infty e^{-\rho t} \big[ p\,x(t) - C(x(t)) \big] dt,
\qquad C(x)=\tfrac{1}{2} c x^2
$$

- State dynamics and initial stock
$$
\dot S(t) = -x(t),\quad S(t)\ge0,\quad S(0)=S_0
$$

With price $p$ constant and $C'(x)=c x$, the current-value Hamiltonian is
$$
H(S,x,\lambda) = p\,x - \tfrac{1}{2} c x^2 + \lambda (-x) = (p-\lambda)x - \tfrac{1}{2} c x^2.
$$

FOCs (Pontryagin) give
$$
\begin{aligned}
&\text{Control: } &&\frac{\partial H}{\partial x} = p - c x - \lambda = 0 \;\Rightarrow\; x(t) = \frac{p-\lambda(t)}{c} \;\; (\ge 0) \\
&\text{Co-state: } &&\dot\lambda(t) = \rho\,\lambda(t) \Rightarrow \lambda(t) = \lambda_0 e^{\rho t} \\
&\text{State: } &&\dot S(t) = -x(t)
\end{aligned}
$$

Because $H$ does not depend on $S$, the rent (shadow price) grows at the interest rate: $\dot\lambda/\lambda = \rho$. The optimal extraction policy is therefore
$$
\boxed{\;x(t) = \max\Big\{\frac{p - \lambda_0 e^{\rho t}}{c},\,0\Big\}\;}
$$
Extraction continues until $\lambda$ reaches $p$ (see the numerator of the optimal policy); denote the stopping time $T^*$, defined by $\lambda(T^*)=p$. This yields
$$
\boxed{\;T^* = \frac{1}{\rho}\,\log\Big(\frac{p}{\lambda_0}\Big)\;}
$$



### Numerical calibration (the script and figure)

Using the parameters $p=1$, $\rho=0.05$, $S_0=10$ and setting $c=0.5$ for the quadratic cost slope (so $C(x)=\tfrac{1}{2} c x^2$ and $MC=c x$), the implied values are
$$
\lambda_0 \approx 0.4488,\qquad T^* \approx 16.02\,\text{time units}.
$$
The trajectories are
$$
\lambda(t) = \lambda_0 e^{\rho t},\qquad x(t)=\frac{p-\lambda(t)}{c}\;\;\text{for }t\le T^*,\qquad S(t)=S_0-\int_0^t x(u)\,du.
$$
The Hotelling rule is visible directly as a constant growth rate
$$
\frac{\dot\lambda(t)}{\lambda(t)} = \rho.
$$

Below is the numerically generated figure from `extraction_sim.R` (saved alongside this file) illustrating $x(t)$, $S(t)$, $\lambda(t)$, and the constant rent growth $\dot\lambda/\lambda=\rho$:

![](extraction_sim.png)

Key takeaways:

- The optimal policy extracts while $\lambda(t)<p$, with $x(t)$ declining to zero at $T^*$ as rents rise exponentially.
- Rents $\lambda$ grow at the interest rate $\rho$ (Hotelling), so the net price equals the scarcity rent and appreciates like a riskless asset.
- Choosing $\lambda_0$ to satisfy the stock constraint ensures complete exhaustion at $T^*$.







